{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ana Carolina Junger\n",
    "Atividade 2 INF2137"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Definição do Problema\n",
    "\n",
    "O Dataset utilizado neste projeto é o Fake.Br Corpus, o primeiro corpus de fake news em português [Monteiro et al., 2018]. As informações foram extraídas do seu projeto no github (https://github.com/roneysco/Fake.br-Corpus). Nele, temos 3 versões do corpus: a primeira com os textos por completo + seus metadados, a segunda com os textos já pre processados pelos autores e a terceira com os pares de fake e true news truncados para terem o mesmo tamanho. Nessa segunda parte do projeto, iremos avaliar os dados pré processados."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Iniciando o notebook + imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as ms # para tratamento de missings\n",
    "from matplotlib import cm\n",
    "from pandas import set_option\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Carregando o Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse arquivo já está pre-processado, normalizando as palavras, removendo stopwords, acentos etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/roneysco/Fake.br-Corpus/master/preprocessed/pre-processed.csv\"\n",
    "\n",
    "df_orig = pd.read_csv(url, skiprows=0, delimiter=',', usecols=[\"label\", \"preprocessed_news\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_orig.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79541"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X_processed = vectorizer.fit_transform(df[\"preprocessed_news\"])\n",
    "\n",
    "tfidfconverter = TfidfVectorizer(max_features=800000)\n",
    "X_processed_Tfid = tfidfconverter.fit_transform(df[\"preprocessed_news\"])\n",
    "\n",
    "len(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Modelos de Classificação"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Já é sabido pelo modelo que o SVM performa muito bem com o bag-of-words, dessa forma iremos avaliar o variações do bag-of-words em diferentes modelos. De maneira analoga, também iremos avaliar o TFIDF."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definição da classe do Pre Processador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definindo o corte e a semente \n",
    "test_size = 0.30\n",
    "seed = 7\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed.toarray(), df[\"label\"], test_size=test_size, random_state=seed)\n",
    "\n",
    "\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# # X_train_transform, X_test_transform, y_train_transform, y_test_transform = pre_processador.pre_processar(df, test_size)\n",
    "# # Parâmetros e partições da validação cruzada\n",
    "scoring = 'accuracy'\n",
    "num_particoes = 10\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=num_particoes, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianNB</label><div class=\"sk-toggleable__content\"><pre>GaussianNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# definindo o corte e a semente \n",
    "test_size = 0.30\n",
    "seed = 7\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed_Tfid.toarray(), df[\"label\"], test_size=test_size, random_state=seed)\n",
    "\n",
    "\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# # # X_train_transform, X_test_transform, y_train_transform, y_test_transform = pre_processador.pre_processar(df, test_size)\n",
    "# # # Parâmetros e partições da validação cruzada\n",
    "# scoring = 'accuracy'\n",
    "# num_particoes = 10\n",
    "\n",
    "# kfold = StratifiedKFold(n_splits=num_particoes, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8023148148148148\n",
      "[[753 345]\n",
      " [ 82 980]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fake       0.90      0.69      0.78      1098\n",
      "        true       0.74      0.92      0.82      1062\n",
      "\n",
      "    accuracy                           0.80      2160\n",
      "   macro avg       0.82      0.80      0.80      2160\n",
      "weighted avg       0.82      0.80      0.80      2160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test)\n",
    "print(accuracy_score(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisando os melhores parametros para cada modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variações \n",
    "\n",
    "# definindo o corte e a semente \n",
    "test_size = 0.20\n",
    "seed = 7\n",
    "# Parâmetros e partições da validação cruzada\n",
    "scoring = 'accuracy'\n",
    "num_particoes = 10\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=num_particoes, shuffle=True, random_state=seed)\n",
    "\n",
    "max_features = [1000, 5000, 10000] # maximo que roda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processo de GridSearch \n",
    "\n",
    "models = []\n",
    "\n",
    "# Navie Bayes \n",
    "param_grid = {'var_smoothing': np.logspace(0,-9, num=20)}\n",
    "models.append((\"Navie Bayes\", GaussianNB(), param_grid))\n",
    "\n",
    "\n",
    "# Decision Tree\n",
    "criterion = ['gini','entropy']\n",
    "max_depth = [3,4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150]\n",
    "param_grid = {'criterion': criterion,'max_depth': max_depth}\n",
    "models.append((\"Decision Tree\", DecisionTreeClassifier(), param_grid))\n",
    "\n",
    "\n",
    "#  KNN\n",
    "k = [1,3,5,7,9,11,13,15,17,19,21]\n",
    "distancias = [\"euclidean\", \"manhattan\", \"minkowski\"]\n",
    "param_grid = dict(n_neighbors=k, metric=distancias)\n",
    "models.append((\"KNN\", KNeighborsClassifier(), param_grid))\n",
    "\n",
    "# SVM\n",
    "param_grid = {\n",
    "  'C': [0.1, 0.3, 0.5, 0.7, 0.9, 1.0, 1.3, 1.5, 1.7, 2.0],\n",
    "  'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "}\n",
    "models.append((\"SVM\", SVC(), param_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAG-OF-WORDS MAX-FEATURE:  1000\n",
      "Melhor Navie Bayes : 0.947917 usando {'var_smoothing': 1.8329807108324375e-05}\n",
      "Melhor Decision Tree : 0.907986 usando {'criterion': 'entropy', 'max_depth': 150}\n",
      "Melhor KNN : 0.807639 usando {'metric': 'euclidean', 'n_neighbors': 1}\n",
      "Melhor SVM : 0.961806 usando {'C': 2.0, 'kernel': 'rbf'}\n",
      "TFIDF MAX-FEATURE:  1000\n",
      "Melhor Navie Bayes : 0.839410 usando {'var_smoothing': 0.012742749857031341}\n",
      "Melhor Decision Tree : 0.902778 usando {'criterion': 'gini', 'max_depth': 7}\n",
      "Melhor KNN : 0.734375 usando {'metric': 'euclidean', 'n_neighbors': 1}\n",
      "Melhor SVM : 0.958681 usando {'C': 2.0, 'kernel': 'rbf'}\n",
      "BAG-OF-WORDS MAX-FEATURE:  5000\n",
      "Melhor Navie Bayes : 0.942187 usando {'var_smoothing': 1.8329807108324375e-05}\n",
      "Melhor Decision Tree : 0.910069 usando {'criterion': 'entropy', 'max_depth': 12}\n",
      "Melhor KNN : 0.760938 usando {'metric': 'euclidean', 'n_neighbors': 1}\n",
      "Melhor SVM : 0.962847 usando {'C': 1.7, 'kernel': 'rbf'}\n",
      "TFIDF MAX-FEATURE:  5000\n",
      "Melhor Navie Bayes : 0.819097 usando {'var_smoothing': 0.0379269019073225}\n",
      "Melhor Decision Tree : 0.901563 usando {'criterion': 'gini', 'max_depth': 7}\n",
      "Melhor KNN : 0.726910 usando {'metric': 'euclidean', 'n_neighbors': 1}\n",
      "Melhor SVM : 0.959896 usando {'C': 0.7, 'kernel': 'linear'}\n",
      "BAG-OF-WORDS MAX-FEATURE:  10000\n",
      "Melhor Navie Bayes : 0.935590 usando {'var_smoothing': 1.8329807108324375e-05}\n",
      "Melhor Decision Tree : 0.909028 usando {'criterion': 'entropy', 'max_depth': 12}\n",
      "Melhor KNN : 0.746354 usando {'metric': 'euclidean', 'n_neighbors': 1}\n",
      "Melhor SVM : 0.962674 usando {'C': 2.0, 'kernel': 'rbf'}\n",
      "TFIDF MAX-FEATURE:  10000\n",
      "Melhor Navie Bayes : 0.821181 usando {'var_smoothing': 0.0379269019073225}\n",
      "Melhor Decision Tree : 0.900347 usando {'criterion': 'entropy', 'max_depth': 11}\n",
      "Melhor KNN : 0.723437 usando {'metric': 'euclidean', 'n_neighbors': 1}\n",
      "Melhor SVM : 0.958160 usando {'C': 2.0, 'kernel': 'rbf'}\n",
      "BAG-OF-WORDS MAX-FEATURE:  50000\n",
      "Melhor Navie Bayes : 0.898958 usando {'var_smoothing': 6.158482110660267e-06}\n",
      "Melhor Decision Tree : 0.910417 usando {'criterion': 'entropy', 'max_depth': 40}\n"
     ]
    }
   ],
   "source": [
    "for max_feature in max_features:\n",
    "  bag_of_words = CountVectorizer(max_features=max_feature)\n",
    "  X_processed = bag_of_words.fit_transform(df[\"preprocessed_news\"])\n",
    "  \n",
    "  X_train, X_test, y_train, y_test = train_test_split(X_processed.toarray(), df[\"label\"], test_size=test_size, random_state=seed)\n",
    "  print(\"BAG-OF-WORDS MAX-FEATURE: \", max_feature)\n",
    "  \n",
    "  for name, model, param_grid in models:\n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid, \n",
    "                      scoring=scoring, cv=kfold)\n",
    "    grid.fit(X_train, y_train) \n",
    "\n",
    "    # imprime o melhor resultado\n",
    "    print(\"Melhor %s : %f usando %s\" % \n",
    "          (name, grid.best_score_, grid.best_params_))\n",
    "    \n",
    "    \n",
    "  tfidfconverter = TfidfVectorizer(max_features=max_feature)\n",
    "  X_processed_Tfid = tfidfconverter.fit_transform(df[\"preprocessed_news\"])\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X_processed_Tfid.toarray(), df[\"label\"], test_size=test_size, random_state=seed)\n",
    "  print(\"TFIDF MAX-FEATURE: \", max_feature)\n",
    "  \n",
    "  \n",
    "  for name, model, param_grid in models:\n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid, \n",
    "                      scoring=scoring, cv=kfold)\n",
    "    grid.fit(X_train, y_train) \n",
    "\n",
    "    # imprime o melhor resultado\n",
    "    print(\"Melhor %s : %f usando %s\" % \n",
    "          (name, grid.best_score_, grid.best_params_))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BAG-OF-WORDS MAX-FEATURE:  1000\n",
    "Melhor Navie Bayes : 0.947917 usando {'var_smoothing': 1.8329807108324375e-05}\n",
    "Melhor Decision Tree : 0.907986 usando {'criterion': 'entropy', 'max_depth': 150}\n",
    "Melhor KNN : 0.807639 usando {'metric': 'euclidean', 'n_neighbors': 1}\n",
    "Melhor SVM : 0.961806 usando {'C': 2.0, 'kernel': 'rbf'}\n",
    "TFIDF MAX-FEATURE:  1000\n",
    "Melhor Navie Bayes : 0.839410 usando {'var_smoothing': 0.012742749857031341}\n",
    "Melhor Decision Tree : 0.902778 usando {'criterion': 'gini', 'max_depth': 7}\n",
    "Melhor KNN : 0.734375 usando {'metric': 'euclidean', 'n_neighbors': 1}\n",
    "Melhor SVM : 0.958681 usando {'C': 2.0, 'kernel': 'rbf'}\n",
    "BAG-OF-WORDS MAX-FEATURE:  5000\n",
    "Melhor Navie Bayes : 0.942187 usando {'var_smoothing': 1.8329807108324375e-05}\n",
    "Melhor Decision Tree : 0.910069 usando {'criterion': 'entropy', 'max_depth': 12}\n",
    "Melhor KNN : 0.760938 usando {'metric': 'euclidean', 'n_neighbors': 1}\n",
    "Melhor SVM : 0.962847 usando {'C': 1.7, 'kernel': 'rbf'}\n",
    "TFIDF MAX-FEATURE:  5000\n",
    "Melhor Navie Bayes : 0.819097 usando {'var_smoothing': 0.0379269019073225}\n",
    "Melhor Decision Tree : 0.901563 usando {'criterion': 'gini', 'max_depth': 7}\n",
    "Melhor KNN : 0.726910 usando {'metric': 'euclidean', 'n_neighbors': 1}\n",
    "Melhor SVM : 0.959896 usando {'C': 0.7, 'kernel': 'linear'}\n",
    "BAG-OF-WORDS MAX-FEATURE:  10000\n",
    "Melhor Navie Bayes : 0.935590 usando {'var_smoothing': 1.8329807108324375e-05}\n",
    "Melhor Decision Tree : 0.909028 usando {'criterion': 'entropy', 'max_depth': 12}\n",
    "Melhor KNN : 0.746354 usando {'metric': 'euclidean', 'n_neighbors': 1}\n",
    "Melhor SVM : 0.962674 usando {'C': 2.0, 'kernel': 'rbf'}\n",
    "TFIDF MAX-FEATURE:  10000\n",
    "Melhor Navie Bayes : 0.821181 usando {'var_smoothing': 0.0379269019073225}\n",
    "Melhor Decision Tree : 0.900347 usando {'criterion': 'entropy', 'max_depth': 11}\n",
    "Melhor KNN : 0.723437 usando {'metric': 'euclidean', 'n_neighbors': 1}\n",
    "Melhor SVM : 0.958160 usando {'C': 2.0, 'kernel': 'rbf'}\n",
    "BAG-OF-WORDS MAX-FEATURE:  50000\n",
    "Melhor Navie Bayes : 0.898958 usando {'var_smoothing': 6.158482110660267e-06}\n",
    "Melhor Decision Tree : 0.910417 usando {'criterion': 'entropy', 'max_depth': 40}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Algoritimo rodou até 10000 features depois travou o computador, vamos considerar esse o valor máximo possível de manipular\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criação e avaliação dos modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.952 (0.008)\n",
      "KNN: 0.949 (0.012)\n",
      "CART: 0.947 (0.010)\n",
      "NB: 0.918 (0.012)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(7) # definindo uma semente global\n",
    "\n",
    "# armazeando os pipelines e os resultados para todas as visões do dataset\n",
    "# como sao datasets diferentes, pelos tratamentos, irei definir dois pipelines \n",
    "pipelines = []\n",
    "results = []\n",
    "names = []\n",
    "\n",
    "# definindo os parâmetros do classificador base para o ensambles\n",
    "base = DecisionTreeClassifier()\n",
    "num_trees = 100\n",
    "max_features = 3\n",
    "\n",
    "# criando os modelos para o VotingClassifier\n",
    "bases = []\n",
    "model1 = LogisticRegression(max_iter=200)\n",
    "bases.append(('logistic', model1))\n",
    "model2 = DecisionTreeClassifier()\n",
    "bases.append(('cart', model2))\n",
    "model3 = SVC()\n",
    "bases.append(('svm', model3))\n",
    "\n",
    "# Criando os elementos do pipeline\n",
    "\n",
    "# Algoritmos que serão utilizados\n",
    "reg_log = ('LR', LogisticRegression(max_iter=200))\n",
    "knn = ('KNN', KNeighborsClassifier(metric=\"manhattan\", n_neighbors=5))\n",
    "cart = ('CART', DecisionTreeClassifier(criterion=\"gini\", max_depth = 5))\n",
    "naive_bayes = ('NB', GaussianNB(var_smoothing=1e-9))\n",
    "svm = ('SVM', SVC(C = 1.7, kernel = \"linear\"))\n",
    "bagging = ('Bag', BaggingClassifier(base_estimator=base, \n",
    "                                    n_estimators=num_trees))\n",
    "random_forest = ('RF', RandomForestClassifier(n_estimators=num_trees, \n",
    "                                              max_features=max_features))\n",
    "extra_trees = ('ET', ExtraTreesClassifier(n_estimators=num_trees, \n",
    "                                          max_features=max_features))\n",
    "adaboost = ('Ada', AdaBoostClassifier(n_estimators=num_trees))\n",
    "gradient_boosting = ('GB', GradientBoostingClassifier(n_estimators=num_trees))\n",
    "voting = ('Voting', VotingClassifier(bases))\n",
    "\n",
    "\n",
    "# Montando os pipelines\n",
    "\n",
    "# Dataset original\n",
    "pipelines.append(('LR', Pipeline([reg_log]))) \n",
    "pipelines.append(('KNN', Pipeline([knn])))\n",
    "pipelines.append(('CART', Pipeline([cart])))\n",
    "pipelines.append(('NB', Pipeline([naive_bayes])))\n",
    "pipelines.append(('SVM', Pipeline([svm])))\n",
    "pipelines.append(('Bag', Pipeline([bagging])))\n",
    "pipelines.append(('RF', Pipeline([random_forest])))\n",
    "pipelines.append(('ET', Pipeline([extra_trees])))\n",
    "pipelines.append(('Ada', Pipeline([adaboost])))\n",
    "pipelines.append(('GB', Pipeline([gradient_boosting])))\n",
    "pipelines.append(('Vot', Pipeline([voting])))\n",
    "\n",
    "\n",
    "# Executando os pipelines - datasets SEM transformações\n",
    "for name, model in pipelines:\n",
    "    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %.3f (%.3f)\" % (name, cv_results.mean(), cv_results.std()) \n",
    "    print(msg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os resultados foram muito positivos, isso pode ser por conta de algum viés na forma de escrite de fakenews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
